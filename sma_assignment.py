# -*- coding: utf-8 -*-
"""SMA assignment

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hw6yZLEtXC6A4iZ4Hc2Prv2UYV1VM79U
"""

import os

if not os.path.exists("my_new_directory"):
    os.mkdir("my_new_directory")
else:
    print("Directory 'my_new_directory' already exists.")

import os
import pandas as pd
import json
import matplotlib.pyplot as plt

import folium
from folium.plugins import HeatMap
dir_path = "my_new_directory" # Using the directory created in the previous cell
files = os.listdir(dir_path)
print("Total no. of files:", len(files))

with open(os.path.join(dir_path, files[0])) as f:
    json_object = json.load(f)

print(len(json_object))

keys = list(json_object[0].keys())
print("The keys in data are:", keys)

df_main = pd.DataFrame(data = {}, columns = keys)
df_main.columns[df_main.columns.duplicated()].to_list()

for i in range(len(files)):
  with open(os.path.join(dir_path, files[i])) as f:
    json_obj = json.load(f)
    df_main = pd.concat([df_main, pd.DataFrame(json_obj)], ignore_index=True)

print(df_main.shape)
df = df_main.copy()
df.head()

df_map = df[['created_at', 'location', 'latitude', 'longitude', 'altitude', 'province']]
df_map.head()
print(df_map.shape)

df_map.dropna(subset=['longitude', 'latitude'], inplace=True)

df_map = df_map[(df_map['latitude'] >= -90) & (df_map['latitude'] <= 90)]
df_map = df_map[(df_map['longitude'] >= -180) & (df_map['longitude'] <= 180)]

df_map['latitude'] = df_map['latitude'].round(4)
df_map['longitude'] = df_map['longitude'].round(4)

df_map.shape

df_group = df_map.drop(['location' ,'altitude','province'], axis='columns')

map_center = [df_group['longitude'].mean(), df_group['latitude'].mean()]
heat_map = folium.Map(locations = map_center, zoom_start=4)

head_data = df_group[['latitude', 'longitude']].values.tolist()
HeatMap(head_data, radius=10, blur=15, max_zoom=6).add_to(heat_map)

image = "dutch_heatmap.html"
heat_map.save(image)
print(df['full_text'].head())
print(df['text_translation'].head())

df_comments = df[['text_translation']]
import nltk

nltk.download([
"names",
"stopwords",
"state_union",
"twitter_samples",
"movie_reviews",
"averaged_perceptron_tagger",
"vader_lexicon",
"punkt",
])

from nltk.sentiment import SentimentIntensityAnalyzer
sia = SentimentIntensityAnalyzer()
df_comments.head()
df_comments.dropna(inplace=True)

def senti_analysis(text):
    scores = sia.polarity_scores(text)
    return scores['compound']
df_comments['sentiment_scores'] = df_comments['text_translation'].apply(senti_analysis)

def sentiments(scores):
    if scores > 0.05:
        return "positive"

    elif scores <= -0.05:
        return "negative"

    else:
        return "neutral"
df_comments['sentiment_label'] = df_comments['sentiment_scores'].apply(sentiments)

df_comments.to_csv("sentimentAnalysis.csv", index = False)