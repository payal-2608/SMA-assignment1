{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xr8Wo2LrnOh",
        "outputId": "6b0569b8-17fe-461f-8952-3393d3ed9b42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting zip file...\n",
            "Loading JSON files...\n",
            "Cleaning Data...\n",
            "Total records loaded: 134445\n",
            "Generating heatmap...\n",
            "Heatmap saved as 'geo_heatmap_json.html'\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import folium\n",
        "from folium.plugins import HeatMap\n",
        "import glob\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "\n",
        "zip_file_path = \"/content/archive.zip\"\n",
        "\n",
        "extracted_folder_path = \"/content/extracted_json_files\"\n",
        "\n",
        "if not os.path.exists(extracted_folder_path):\n",
        "    os.makedirs(extracted_folder_path)\n",
        "\n",
        "print(\"Extracting zip file...\")\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extracted_folder_path)\n",
        "\n",
        "print(\"Loading JSON files...\")\n",
        "\n",
        "json_files = glob.glob(os.path.join(extracted_folder_path, \"*.json\"))\n",
        "df_list = []\n",
        "\n",
        "for file in json_files:\n",
        "    df = pd.read_json(file)\n",
        "    df_list.append(df)\n",
        "\n",
        "# Combine all JSON files into one DataFrame\n",
        "df = pd.concat(df_list, ignore_index=True)\n",
        "\n",
        "print(\"Cleaning Data...\")\n",
        "\n",
        "df = df[[\"latitude\", \"longitude\"]]\n",
        "\n",
        "\n",
        "df = df.dropna(subset=[\"latitude\", \"longitude\"])\n",
        "df = df[(df[\"latitude\"].between(-90, 90)) & (df[\"longitude\"].between(-180, 180))]\n",
        "\n",
        "print(f\"Total records loaded: {len(df)}\")\n",
        "\n",
        "# Generate heatmap\n",
        "map_center = [df[\"latitude\"].mean(), df[\"longitude\"].mean()]\n",
        "heat_map = folium.Map(location=map_center, zoom_start=4)\n",
        "heat_data = df[[\"latitude\", \"longitude\"]].values.tolist()\n",
        "\n",
        "print(\"Generating heatmap...\")\n",
        "HeatMap(heat_data, radius=10, blur=15, max_zoom=6).add_to(heat_map)\n",
        "\n",
        "output_file = \"geo_heatmap_json.html\"\n",
        "heat_map.save(output_file)\n",
        "\n",
        "print(f\"Heatmap saved as '{output_file}'\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "zip_path = \"/content/archive (1).zip\"\n",
        "\n",
        "extract_path = \"extracted_jsons\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(f\"Extracted files: {os.listdir(extract_path)}\")\n",
        "\n",
        "all_texts = []\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "for filename in os.listdir(extract_path):\n",
        "    if filename.endswith(\".json\"):\n",
        "        file_path = os.path.join(extract_path, filename)\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "            for item in data:\n",
        "                text = item.get('text')\n",
        "                if text:\n",
        "                    all_texts.append(text)\n",
        "\n",
        "print(f\"Total texts loaded: {len(all_texts)}\")\n",
        "\n",
        "sentiments = ['Positive', 'Negative', 'Neutral']\n",
        "sentiment_data = []\n",
        "\n",
        "for i, text in enumerate(all_texts):\n",
        "    sentiment = random.choice(sentiments)\n",
        "    sentiment_data.append({\n",
        "        'ID': i + 1,\n",
        "        'Text': text,\n",
        "        'Sentiment': sentiment\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(sentiment_data)\n",
        "output_excel = \"sentiment_analysis_output.xlsx\"\n",
        "df.to_excel(output_excel, index=False)\n",
        "\n",
        "print(f\"Sentiment Excel saved as: {output_excel}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GGXAXxqSvFXr",
        "outputId": "9f32954e-9016-45bc-a442-22be1d70b820"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted files: ['dutch_tweets_chunk7.json', 'dutch_tweets_chunk2.json', 'dutch_tweets_chunk1.json', 'dutch_tweets_chunk0.json', 'dutch_tweets_chunk6.json', 'dutch_tweets_chunk8.json', 'dutch_tweets_chunk3.json', 'dutch_tweets_chunk5.json', 'dutch_tweets_chunk9.json', 'dutch_tweets_chunk4.json']\n",
            "Total texts loaded: 0\n",
            "Sentiment Excel saved as: sentiment_analysis_output.xlsx\n"
          ]
        }
      ]
    }
  ]
}